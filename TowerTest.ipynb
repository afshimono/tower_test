{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d75238-88c4-480b-9246-b0fd15e0bbc4",
   "metadata": {},
   "source": [
    "## Brief Problem Description\n",
    "Given an input file which has the location of a person based on cellular tower triangulation, generate an output with 2 columns,\n",
    "containing one row for each input line, tagging it as a tower jump (erroneous localization) and a confidence level (expressed as a number between 0 and 1)\n",
    "\n",
    "## References\n",
    "\n",
    "https://www.tinybird.co/blog-posts/anomaly-detection\n",
    "\n",
    "https://medium.com/analytics-vidhya/removing-outliers-understanding-how-and-what-behind-the-magic-18a78ab480ff\n",
    "\n",
    "\n",
    "## Solution (TLDR)\n",
    "\n",
    "1. if lat and lon are zero -> flag as jump ( I hope John Doe is not visiting [Null Island](https://en.wikipedia.org/wiki/Null_Island) )\n",
    "2. order the full dataset by datetime\n",
    "3. calculate diff in minutes between points\n",
    "4. calculate quartile of 80% in min (this was selected by me at random, it will be a variable in case we want to play with it)\n",
    "5. group measurements in cluster when the time in minutes between rows is below the 80% quartile\n",
    "6. remove outliers, calculate z value for lat and lon, any row that has either lon or lat below z +- 2 std, will be flagged as jump\n",
    "7. calculate confidence as how close the lat and lon are to z if it is not a jump, or 1 - diff if it was flagged as jump\n",
    "8. reorder output based on page and item number\n",
    "\n",
    "## This Notebook\n",
    "... was used when attempting to extract relevant data from the sample to write the solution, so it is not necessarily in an order\n",
    "to help understand the solution, but in the order the data was being analysed.\n",
    "\n",
    "## Initial Assumptions\n",
    "Given the nature of the problem, which does not appear to have an exact solution that can be solved by a formal algorithm (the request to use a \"confidence level\" in the output gave that hint), we will try some statistical approach.\n",
    "1- This appears to be a particular case of anomaly detection, and it will be handled as such\n",
    "2- Many simplifications will be done given the limitations of time\n",
    "3- A Script will be provided that can be called on the command line to produce the output, given the input is in the same format that the one provided in the challenge description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb2b4e-2d64-4897-b4cb-61c31a78de84",
   "metadata": {},
   "source": [
    "### Loading input\n",
    "First load the input file and check the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7acf01-8817-4903-adab-f20214ac16c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "INPUT_PATH = \"TowerJumpsDataSet_CarrierRecords.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf21854-5eb0-4fd7-a0df-399ec453ec34",
   "metadata": {},
   "source": [
    "Now, we look at all values in the columns to try to get a perspective of how this data is distributed.\n",
    "By running `describe`, nothing particular draws our attention, except that the page number and item number do not seem relevant to our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f512a96-1265-4a27-844a-43f96865ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e20f2ca-e3bc-4ceb-80a2-65c92a6c5e07",
   "metadata": {},
   "source": [
    "Time zone appears to have 2 exotic values: Etc and Asia. LetÂ´s look a bit closer at what data they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abda81a-d705-478a-b4cc-273b6eb4449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Time Zone\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3321b7f9-3def-43b3-849b-4994312e5d32",
   "metadata": {},
   "source": [
    "Asia has only 5 entries, the location (lat and lon) might be correct, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff5bcd-a4db-492f-9eca-13fcb201f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Time Zone\"]==\"Asia/Bishkek\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ec9de-c0e7-4fe9-86cf-a26e73875dda",
   "metadata": {},
   "source": [
    "ETC has no good value for lat and lon, so this one can possibly be a case for automatically flagging it as a jump with high confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a16abc-f92b-49ad-8c02-c209a84ecdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Time Zone\"]==\"Etc/GMT\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795522a-818f-4b0e-9c34-43e226643a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Time Zone\"]==\"Etc/GMT\"][[\"Latitude\",\"Longitude\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763bc81-61d7-4a41-a1f3-09e8cd577fcb",
   "metadata": {},
   "source": [
    "Country is either USA or NaN, so nothing extraordinary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d8804-41e0-4d55-b64b-ba11503d8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Country\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d028d1-6672-4b6d-b333-3f9b5ecd4b55",
   "metadata": {},
   "source": [
    "States look OK, except for the `unknown` value. However, some of the `unknown` have non zero lat/lon location, so this alone is possibly not a good indicator of a jump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22ee71e-a690-4e9e-bec0-2e5f712795b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"State\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd01c51-0df6-4350-a026-c2ab6679f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"State\"]==\"unknown\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b3127-3a9d-4bf6-acc6-9bae3b516355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"State\"]==\"unknown\"][[\"Latitude\",\"Longitude\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829159c-0f4a-4018-8c4e-3f275e9b7f1e",
   "metadata": {},
   "source": [
    "Record Type does not seem to give valuable information for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808cd2b-a2e1-4455-a47c-ba87140656c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Record Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81ebbc-c037-4199-9870-62dd9461833a",
   "metadata": {},
   "source": [
    "Datetime can be used to gather interesting information, like the distribution of the records over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3930e90-fbe3-4eff-ac7e-8a4a402517cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Datetime\"] = pd.to_datetime(df[\"Local Date & Time\"], format=\"%m/%d/%y %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f49870-b65e-442a-a07c-0590cb1359bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=[\"Datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9b80a-baea-4908-9701-6fe208197b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Datetime\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97331d5c-11ea-4811-9212-b8b540f8c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansed_df = df[df[\"Datetime\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d4c41-961a-4b82-aef7-93f8edc165b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DatetimeDelta\"] =  cleansed_df[\"Datetime\"] - cleansed_df[\"Datetime\"].shift(1)\n",
    "df[\"DatetimeDelta\"] = df[\"DatetimeDelta\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b61e2-748c-4289-8ade-1de3ea70003e",
   "metadata": {},
   "source": [
    "## DatetimeDeltaMinutes\n",
    "This is an interesting metric because it shows the distribution of data.\n",
    "In practise, this means that the effect that many records with the same location happen in a short period of time, in bursts,\n",
    "is very common in this dataset.\n",
    "I do not have any expertise in cellular tower location algorithms, but statiscally, we will consider that if a position is reported\n",
    "repeatedly, this will mean it is more relevant and has a higher chance of being correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d91ceb-ca74-4511-8a95-ea8dc4a45eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DatetimeDeltaMinutes\"] = df[\"DatetimeDelta\"].apply(lambda x: (x.total_seconds())/60 if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c855987-0c12-4c68-bca1-56e8be57ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Datetime\",\"DatetimeDelta\",\"DatetimeDeltaMinutes\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8266ca-714d-413b-ac10-597cb28047c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DatetimeDeltaMinutes\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c8d06-48ff-4f62-a83a-c5fdb808197b",
   "metadata": {},
   "source": [
    "Now we can visualize that the vast majority of interval between measurements is actually pretty small, and also that\n",
    "the 90% quartile is 16 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe2d41-bc5a-402f-928b-0c7e9ce28257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(data=df[[\"Datetime\",\"DatetimeDeltaMinutes\"]], x=\"DatetimeDeltaMinutes\",binwidth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b7c4a3-b348-4ebb-9c1e-b4d0d20dc1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DatetimeDeltaMinutes\"].quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6eefa5-6fb8-4010-a61b-7e839adda01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quartile_limit = df[\"DatetimeDeltaMinutes\"].quantile(0.8)\n",
    "quartile_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba373e-a90e-40e8-8132-805532f5be58",
   "metadata": {},
   "source": [
    "## Intuition and Main code\n",
    "Reading the references regarding anomaly detection, and taking into account the pecualirity of ths data set regarding the\n",
    "\"burst\" effect (there are many measurements very close to each other in time, and those groups are separated by a large amount of time), instead of using a fixed time window to calculate the Z value each point and judge if a point is an anomaly or not,\n",
    "we will instead cluster the points together when they happen close to each other in time.\n",
    "\n",
    "The idea is that if a measurement happens in a single geo location and there are no conflicting measurements at the same time, we do not have a lot of reason to believe it is faulty, but if many measurements happen with different locations and close to each other in terms of time, the chance that some of them are faulty is higher.\n",
    "We have one specific case that is always tagged as faulty which is lat and lon = (0,0).\n",
    "\n",
    "There were some parameter tuning made while looking at the results to optimize the desired behavior.\n",
    "1. the value used to classify a point as either faulty or not was 1 standard deviation. This is lower than the suggested value of 2 because we want to have one tagged as correct and many different tagged as faulty when there are conflicts.\n",
    "2. the upper and lower bounds used to remove outliers were 15 quantile and 85 quantile, because they had better result than 25 and 75.\n",
    "3. the calculation of accuracy was done manually, following the logic that z values close to 0 should have higher accuracy, whereas values higher than 2 should have very low accuracy. At the same time, if we flag a point as faulty and the z value is really big, accuracy should be higher.\n",
    "4. the calculation of difference between locations was done using simple euclidean distance instead of using proper geolocation calculation. This was required to calculate the Z Score for each point.\n",
    "\n",
    "Another important aspect we took into account is that faulty measurements should be the exception, not the rule, otherwise this method of geolocation would make sense. In other words, there should be generally more correct measurements than faulty ones.\n",
    "\n",
    "With those concepts in mind, the following code was written to calculate the Z Score to each point and tag it as `tower_jump` = y or n.\n",
    "\n",
    "**Note that the code below will run only on 1000 entries. This was done on purpose to just validate the code. The full version of the code is available in the `main.py` file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ec5dd-6033-4119-aa8f-9cd7b6410906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "class LocationPoint:\n",
    "    def __init__(self, location:Tuple[float,float], page:int, item:int, date_and_time:str):\n",
    "        self.location = location\n",
    "        self.page = page\n",
    "        self.item = item\n",
    "        self.date_and_time = date_and_time\n",
    "        self.z_dist = None\n",
    "        self.tower_jump = None\n",
    "        self.accuracy = None\n",
    "        self.cluster_avg = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Location: {self.location} \\n Date_and_time: {self.date_and_time} \\n Page: {self.page} \\n Item: {self.item} \\n Z_dist: {self.z_dist} \\n Tower_jump: {self.tower_jump} \\n Accuracy: {self.accuracy} \\n Cluster AVG: {self.cluster_avg})\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"page\":self.page,\n",
    "            \"item\":self.item,\n",
    "            \"date_and_time\":self.date_and_time,\n",
    "            \"tower_jump\": self.tower_jump,\n",
    "            \"accuracy\": \"{:.2f}\".format(float(self.accuracy))\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def from_row(input):\n",
    "        lat = input[\"Latitude\"]\n",
    "        lon = input[\"Longitude\"]\n",
    "        if isinstance(lat, np.float64):\n",
    "            lat = float(lat)\n",
    "        if isinstance(lon, np.float64):\n",
    "            lon = float(lon)\n",
    "        return LocationPoint((lat,lon),input[\"Page Number\"],input[\"Item Number\"],input[\"Local Date & Time\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04634354-22f3-48f9-ad94-83b526bdf422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_from_z(z_score:float)->int:\n",
    "    if abs(z_score) <= 0.5:\n",
    "        return 80 + (0.5 - abs(z_score))*38\n",
    "    elif abs(z_score) <= 1:\n",
    "        return 50 + (1 - abs(z_score))*30\n",
    "    elif abs(z_score) <= 2:\n",
    "        return 30 + (2 - abs(z_score))*10\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd798183-8103-4259-8839-383aa54471c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def calculate_mean_std_and_z_scores(input_list:List[LocationPoint]) -> List[LocationPoint]:\n",
    "\n",
    "    all_zeros = all([item.location == (0,0) for item in input_list])\n",
    "\n",
    "    if not all_zeros:\n",
    "        lat_list = []\n",
    "        lon_list = []\n",
    "        for item in input_list:\n",
    "            if item.location != (0,0):\n",
    "                lat_list.append(item.location[0])\n",
    "                lon_list.append(item.location[1])\n",
    "    \n",
    "        #print(f\"LAT {lat_list} LON {lon_list}\")\n",
    "        # remove outliers\n",
    "        q1_lat = np.percentile(lat_list, 15)\n",
    "        q3_lat = np.percentile(lat_list, 85)\n",
    "        q1_lon = np.percentile(lon_list, 15)\n",
    "        q3_lon = np.percentile(lon_list, 85)\n",
    "    \n",
    "        # print(f\"PERCENTILE LAT {q1_lat} {q3_lat} LON {q1_lon} {q3_lon}\")\n",
    "    \n",
    "        clean_lat_list = [item for item in lat_list if item >= q1_lat and item <= q3_lat]\n",
    "        clean_lon_list = [item for item in lon_list if item >= q1_lon and item <= q3_lon]\n",
    "        #print(f\"CLEAN LAT {clean_lat_list} LON {clean_lon_list}\")\n",
    "    \n",
    "        lat_avg = np.average(clean_lat_list)\n",
    "        lon_avg = np.average(clean_lon_list)\n",
    "    \n",
    "        std_lat = np.std(clean_lat_list)\n",
    "        std_lon = np.std(clean_lon_list)\n",
    "        #print(f\"STD {std_lat} {std_lon}\")\n",
    "    \n",
    "        total_std = np.sqrt(np.sum(np.power([std_lat,std_lon],2)))\n",
    "    \n",
    "        #print(f\"TOTAL STD {total_std}\")\n",
    "\n",
    "    for item in input_list:\n",
    "        if item.location == (0,0):\n",
    "            item.z_dist = -10\n",
    "            item.tower_jump = \"y\"\n",
    "            item.accuracy = 99\n",
    "        else:\n",
    "            item.z_dist = np.linalg.norm(np.subtract(list(item.location),[lat_avg,lon_avg])) / total_std if total_std > 0 else 0\n",
    "            #print(f\"Z_DIST {item.z_dist}\")\n",
    "            item.tower_jump = \"y\" if abs(item.z_dist) > 1 else \"n\"\n",
    "            item.accuracy = calculate_accuracy_from_z(item.z_dist) if item.tower_jump == \"n\" else (100 - calculate_accuracy_from_z(item.z_dist))\n",
    "        item.cluster_avg = (lat_avg,lon_avg) if not all_zeros else None\n",
    "    return input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6409235b-ec80-4fae-8a25-542a3d1a8788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "point_dict_list = []\n",
    "current_cluster = [LocationPoint.from_row(df.iloc[0])]\n",
    "for index, row in islice(df.iterrows(), 1, 1000):\n",
    "    if row[\"DatetimeDeltaMinutes\"] < 5:\n",
    "        current_cluster.append(LocationPoint.from_row(row))\n",
    "    else:\n",
    "        # print(f\" CLUSTER TO ANALYSE {current_cluster}\")\n",
    "        point_list = calculate_mean_std_and_z_scores(current_cluster)\n",
    "        point_dict_list += [item.to_dict() for item in point_list]\n",
    "        current_cluster = [LocationPoint.from_row(row)]\n",
    "#print(f\"FINAL LIST {point_dict_list}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e778e99-3b70-46fe-a03b-5b84a5f587df",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(point_dict_list)\n",
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c340c-f5e5-45d1-a719-9fe0e52cadca",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.sort_values(by=[\"page\",\"item\"],ascending=[True, True])\n",
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09103639-c153-4a46-85b5-e9dee88ba4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0b3bb7-29dc-4a9e-bc85-dd7641a6dca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
